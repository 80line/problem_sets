---
title: 'Psych 251 Problem Set #2'
author: "Mike Frank"
date: "October 9, 2017"
output: html_document
---

This is problem set #2, in which we hope you will practice the packages `tidyr` and `dplyr`. Along with the tutorial we used in class, there are some great cheat sheets from [RStudio](http://www.rstudio.com/resources/cheatsheets/). 

The data set
------------

This data set comes from a replication of [Janiszewski and Uy (2008)](http://dx.doi.org/10.1111/j.1467-9280.2008.02057.x), who investigated whether the precision of the anchor for a price influences the amount of adjustment. (We use these data briefly in the tutorial).

In the data frame, the `Input.condition` variable represents the experimental condition (under the rounded anchor, the rounded anchor, over the rounded anchor). `Input.price1`, `Input.price2`, and `Input.price3` are the anchors for the `Answer.dog_cost`, `Answer.plasma_cost`, and `Answer.sushi_cost` items.

Part 1: Making these data tidy
------------------------------

Load the tidyverse package and the data

```{r prelims}
library(tidyverse)

d <- read.csv("data/janiszewski_rep_cleaned.csv")
```

This data frame is in *wide* format - that means that each row is a participant and there are multiple observations per participant. This data is not *tidy*.

To make this data tidy, we'll do some cleanup. First, remove the columns you don't need, using the verb `select`.

HINT: `?select` and the examples of helper functions will help you be efficient.

```{r select}
d.tidy <- d %>%
  select(WorkerId, Input.condition, starts_with("Answer"))
```

Try renaming some variables using `rename`. A good naming scheme is:

* consistent with case
* consistent with "." or "_" ( "_" is usually preferred)
* concise as will be comprehensible to others

Try using the `%>%` operator as well. So you will be "piping" `d %>% rename(...)`.

```{r rename}
d.tidy <- d.tidy %>%
  rename(workerid = WorkerId,
         condition = Input.condition,
         dog = Answer.dog_cost,
         plasma = Answer.plasma_cost,
         sushi = Answer.sushi_cost)
```

OK, now for the tricky part. Use the verb *gather* to turn this into a *tidy* data frame.

HINT: look for online examples!

```{r gather}
d.tidy <- d.tidy %>%
  gather(item, bet, dog, plasma, sushi)
```

Now *spread* these data back into a wide format data frame.

```{r spread}
d.wide <- d.tidy %>%
  spread(item, bet)
```


Part 2: Manipulating the data 
-----------------------------

NOTE: If you generally use `plyr` package, note that they do not play nicely together so things like the rename function won't work unless you load `dplyr` after `plyr`.

As we said in class, a good thing to do is always to check histograms of the response variable. Do that now, using either regular base graphics (`hist`) or `ggplot`. What can you conclude? 

```{r}
ggplot(d.tidy, aes(x = bet) )  + 
  geom_histogram(bins = 10) + 
  facet_grid(. ~ item, scales="free_x") 
```


OK, now we turn to the actual data anlysis. We'll be using dplyr verbs to *filter*, *group*,*mutate*, and *summarise* the data.

Start by using *summarise* to compute the grand mean bet. (Note that this is the same as taking the grant mean - the value will come later. Right now we're just learning the syntax of that verb.)

```{r}
d.tidy %>% 
  summarise(bet = mean(bet, na.rm=TRUE)) %>%
  knitr::kable(digits = 2)
```


This is a great time to get comfortable with the %>% operator. In brief, %>% allows you to pipe data from one function to another. So if you would have written:

`d <- function(d, other_stuff)`

you can now write:

`d <- d %>% function(other_stufF)`

That doesn't seem like much, but it's cool when you can replace:

`d <- function1(d, other_stuff)
d <- function2(d, lots_of_other_stuff, more_stuff)
d <- function3(d, yet_more_stuff)`

with

`d <- d %>% 
  function1(other_stuff) %>%
  function2(lots_of_other_stuff, more_stuff) %>%
  function3(yet_more_stuff)`

In other words, you get to make a clean list of the things you want to do and chain them together without a lot of intermediate assignments. 

Let's use that capacity to combine summarise with *group_by*, which allows us to break up our summary into groups. Try grouping by item and condition and taking means using *summarise*, chaining these two verbs with %>%.

```{r}
d.tidy %>% 
  group_by(item, condition) %>%
  summarise(bet = mean(bet, na.rm=TRUE)) %>%
  knitr::kable(digits = 2)
```

Are there condition differences?

> Perhaps, but there are obvious item-to-item variations that swamp the condition differences. 

How are we going to plot condition differences? They are fundamentally different magnitudes from one another. Really we need the size of the deviation from the anchor, which means we need the anchor value. Let's go back to the data and add that in.

Take a look at these two complex piped expression.s You don't have to modify it, but see what is being done here with gather, separate and spread. Run each part (e.g. the first verb, the first two verbs, etc.) and after doing each, look at `head(d.tidy)` to see what they do.  

```{r}
# clean up 
d.tidy <- d %>% 
  select(WorkerId, Input.condition, 
         starts_with("Answer"), 
         starts_with("Input")) %>%
  rename(workerid = WorkerId,
         condition = Input.condition,          
         plasma_anchor = Input.price1,
         dog_anchor = Input.price2,
         sushi_anchor = Input.price3,
         dog_cost = Answer.dog_cost,
         plasma_cost = Answer.plasma_cost, 
         sushi_cost = Answer.sushi_cost) 

# now do the gathering and spreading
d.tidy <- d.tidy %>%
  gather(name, cost, 
         dog_anchor, plasma_anchor, sushi_anchor, 
         dog_cost, plasma_cost, sushi_cost) %>%
  separate(name, c("item", "type"), "_") %>%
  spread(type, cost) 
```

Now we can do the same thing as before but look at the relative difference between anchor and estimate. Let's do this two ways: 

* By computing absolute percentage change in price, and 
* By computing $z$-scores over items. ($z$-scores are [standard scores](https://en.wikipedia.org/wiki/Standard_score) where you subtract the mean and divide by the standard deviation. They're a useful way of standardizing data). 

To do the first, use the *mutate* verb to add a percent change column, then comute the same summary as before. 

```{r}
pcts <- d.tidy %>% 
  mutate(pct_change = abs((anchor - cost) / anchor)) %>%
  group_by(item, condition) %>%
  summarise(pct = mean(pct_change, na.rm=TRUE))
```

To do the second, you will need to *group* once by item, then to *ungroup* and  do the same thing as before. NOTE: you can use group_by(..., add=FALSE) to set new grouping levels, also. 

HINT: `scale(x)` returns a complicated data structure that doesn't play nicely with dplyr. try `scale(x)[,1]` to get what you need. 

```{r}
z.scores <- d.tidy %>% 
  group_by(item) %>%
  mutate(z = scale(anchor - cost)[,1]) %>%
  group_by(item, condition, add=FALSE) %>%
  summarise(z = mean(z, na.rm=TRUE))

```


OK, now here comes the end: we're going to plot the differences and see if anything happened. First the percent change:

```{r}
ggplot(pcts, aes(x = item, y = pct, fill=condition)) + 
  geom_bar(position="dodge", stat="identity")  
```

and the $z$-scores:

```{r}
ggplot(z.scores, aes(x = item, y = z, fill=condition)) + 
  geom_bar(position="dodge", stat="identity")
```

What do you you see in this replication?

> We didn't see a straightforward replication of the predicted effect. 

END 
===